{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae04ad03-b07d-43fa-ad61-245e912201b3",
   "metadata": {},
   "source": [
    "<h1>SSLM - Super Small Language Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3216d6bb-8c01-4868-8521-4aef4c646455",
   "metadata": {},
   "source": [
    "<p>Este é um tutorial muito básico sobre como criar um LLM (Large Language Model).<br>\n",
    "Como não é possível criar um LLM em uma máquina comum, sem GPU principalmente, iremos criar um modelo sem ambição\n",
    "de ser o melhor ou que seja utilizável. Nosso modelo servirá apenas como objeto de estudo.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80b5909-b437-4a93-bf28-61b3a8b4bd03",
   "metadata": {},
   "source": [
    "<h2>Etapas</h2>\n",
    "<ol>\n",
    "    <li>Definição de escopo</li>\n",
    "    <li>Definição de dataset</li>\n",
    "    <li>Definição de arquitetura</li>\n",
    "    <li>Treinamento</li>\n",
    "    <li>Testes</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612fa663-f90e-492b-92b1-4a6362a16ac0",
   "metadata": {},
   "source": [
    "<h3 style=\"color: blue\">1. Definição de escopo</h3>\n",
    "<p>A definição do escopo do nosso modelo é muito importante. Ela definirá como iremos trabalhar daqui pra frente.\n",
    "Existem modelos que geram texto, classificam imagens, criam imagens, criam audio e muito mais!\n",
    "Não há um modelo que seja menos complexo de desenvolver, entretanto.<br>Para nosso projeto, iremos criar um SSLM que gera texto, como ChatGPT,\n",
    " Copilot e outros famosos.</p>\n",
    " <br>\n",
    " <br>\n",
    " <p>ESCOPO DO <strong>SSLM</strong>: <b>Geração de texto</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51afedea-b46e-45f1-a924-877dcdeb30a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h3 style=\"color: blue\">2. Definição de dataset</h3>\n",
    "<p>O dataset é a base de treinamento de qualquer modelo. Um dataset de um modelo que gera imagens é um conjunto de matrizes de imagens que já existem. O dataset de um modelo que gera texto é texto. Como um computador não entende o que é uma imagem, texto, audio etc., apenas números (e em formatação binária física), é importante que transformemos esses datasets em números. As linguagens de programação já fazem a parte de transformar números lógicos em sinais elétricos binários. Nossa ideia é apenas transformar textos em números.</p>\n",
    " <br>\n",
    " <p>Para treinar um LLM como o GPT, é necessário um número imenso de texto de treinamento. A OpenAI fez aquisições de livros, revistas, artigos, \n",
    " sites, repositórios públicos de código (GitHub e GitLab), posts públicos em redes sociais, transcrições de vídeos e muitas outras fontes ricas\n",
    " de texto para treinar o GPT. Para nosso exemplo, vamos utilizar apenas um texto grande, mas não demais, para que o computador não trave em sua execução.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b492283-7c31-4d65-87fc-1d32dc9c031a",
   "metadata": {},
   "source": [
    "<h3 style=\"color=green\">Dataset que utilizaremos: Cumprimentos</h3>\n",
    "<p>Esse dataset contém mensagens que correspondem a cumprimentos não necessariamente formais entre pessoas.\n",
    "Nosso SSLM deverá ser capaz de responder a mensagens simples como essas.</p>\n",
    "<p>\n",
    "Olá! Como você está?<br>\n",
    "Oi! Tudo bem com você?<br>\n",
    "Bom dia! Espero que você tenha um ótimo dia.<br>\n",
    "Boa tarde! Como posso te ajudar hoje?<br>\n",
    "Boa noite! Espero que você esteja bem.<br>\n",
    "Oi! É um prazer falar com você.<br>\n",
    "Olá! Em que posso te ajudar?<br>\n",
    "Oi! Espero que você esteja tendo um bom dia.<br>\n",
    "Olá! Como posso ser útil para você hoje?<br>\n",
    "Oi! Que bom te ver por aqui.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68b1c6f-b743-49ce-a95e-2b771d756bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"dataset.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    file = file.readlines()\n",
    "\n",
    "train_dataset = []\n",
    "for line in file:\n",
    "    train_dataset.append(line.replace(\"\\n\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24abc70d-0268-4ef3-bc2e-b3a848ebb526",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7b38e6-f6dd-4021-89de-197beb837728",
   "metadata": {},
   "source": [
    "<h3 style=\"color: blue\">3. Definição de arquitetura</h3>\n",
    "<p>O treinamento do modelo envolve várias etapas fundamentais para preparar os dados e construir um sistema capaz de gerar respostas coerentes. Primeiramente, as frases do dataset são tokenizadas, ou seja, divididas em palavras ou tokens individuais. Cada token recebe um identificador único, que é armazenado em um índice para facilitar o mapeamento entre palavras e IDs.</p>\n",
    "\n",
    "<p>Após a tokenização, as frases são convertidas em sequências de IDs, representando cada token por seu identificador correspondente. Essas sequências são então padronizadas para um comprimento fixo utilizando padding (adicionando valores padrão no final das sequências mais curtas) ou truncamento (removendo tokens excedentes das sequências mais longas). Isso garante que todas as sequências tenham o mesmo tamanho, facilitando o processamento pelo modelo.</p>\n",
    "\n",
    "<p>Uma boa definição de arquitetura é essencial para garantir que o modelo seja capaz de capturar os padrões desejados nos dados e realizar a tarefa proposta com eficiência.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a301bd8-f3df-4676-a345-63ce80f249c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22cf590-82da-4de9-9700-77497c9db570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_token(token : str, contador : int):\n",
    "    if token not in index:\n",
    "        index[token] = contador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b069f15e-a63c-4abb-be83-980b27ec05e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataset: list):\n",
    "    tokenized_dataset = []\n",
    "    for sentence in dataset:\n",
    "        tokenized_dataset.append(sentence.split())\n",
    "\n",
    "    contador = 1\n",
    "    for sentence in tokenized_dataset:\n",
    "        for token in sentence:\n",
    "            index_token(token, contador)\n",
    "            contador += 1\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba22c7-0a10-47db-8b28-2c70ec6f7d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenize(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427eb9d-b699-44f1-a4e5-7f3b3010d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686b88d6-bc0f-43c5-8475-338bc9d2e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c317d1-ee83-4e6d-b0c4-52238e13a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_ids(tokenized_dataset, index):\n",
    "    dataset_ids = []\n",
    "    for sentence in tokenized_dataset:\n",
    "        sentence_ids = [index[token] for token in sentence]\n",
    "        dataset_ids.append(sentence_ids)\n",
    "    return dataset_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81bf3c6-0b40-41a2-8c2d-5af2f9553137",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ids = sentences_to_ids(tokenized_dataset, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09870f65-6be8-4afa-beca-fb72ad3af06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f886bd-fe5e-4bd3-84cf-3d68c1a19dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_length, padding_value=0):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_length:\n",
    "            # Adiciona padding no final\n",
    "            padded_seq = seq + [padding_value] * (max_length - len(seq))\n",
    "        else:\n",
    "            # Trunca a sequência\n",
    "            padded_seq = seq[:max_length]\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e88878-431d-4e9b-8593-51595ca5c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b91ce-33e8-4157-8864-1fbe61d3dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_dataset_ids = pad_sequences(dataset_ids, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71241413-2b80-4ae2-9232-9ca6d8d3663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_dataset_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138dbe09-9c4c-425c-9b8c-63f280806551",
   "metadata": {},
   "source": [
    "<h3 style=\"color: blue\">4. Treinamento</h3>\n",
    "<p>Em seguida, são calculadas as probabilidades de transição entre os tokens com base nas sequências de treinamento. Para isso, o modelo analisa a frequência com que cada token é seguido por outro, gerando um índice de probabilidades normalizadas. Esse índice é essencial para que o modelo aprenda os padrões de transição entre palavras e possa gerar respostas baseadas em mensagens de entrada.</p>\n",
    "\n",
    "<p>Por fim, o modelo utiliza essas probabilidades para gerar respostas. Quando uma mensagem é recebida, o sistema identifica o último token da mensagem e utiliza as probabilidades de transição para prever os próximos tokens, construindo uma resposta de forma iterativa. Esse processo continua até que um token de término seja alcançado ou que a resposta atinja um comprimento máximo predefinido.</p>\n",
    "\n",
    "<p>Essas etapas garantem que o modelo seja capaz de compreender e responder de forma contextualizada, utilizando os padrões aprendidos durante o treinamento.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c868e-01d0-4318-a7c9-2344436d0d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11359ad5-6911-4ab0-a85d-88bb024da23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_token_probabilities(sequences):\n",
    "    transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "    transition_probabilities = {}\n",
    "\n",
    "    # Contar transições entre tokens\n",
    "    for seq in sequences:\n",
    "        for i in range(len(seq) - 1):\n",
    "            current_token = seq[i]\n",
    "            next_token = seq[i + 1]\n",
    "            transition_counts[current_token][next_token] += 1\n",
    "\n",
    "    # Calcular probabilidades normalizadas\n",
    "    for token, next_tokens in transition_counts.items():\n",
    "        total_transitions = sum(next_tokens.values())\n",
    "        transition_probabilities[token] = {\n",
    "            next_token: count / total_transitions\n",
    "            for next_token, count in next_tokens.items()\n",
    "        }\n",
    "\n",
    "    return transition_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca79460-c752-4295-943f-386f0fd0b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_probabilities = calculate_token_probabilities(padded_dataset_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa12737-661a-4af7-b665-d96f23c743d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839ea66-c975-47c1-9e49-63454aa0d82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8315e10-c8ca-49e9-880d-25beb6f43761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(message, index, token_probabilities):\n",
    "    # Tokenizar a mensagem e verificar se a última palavra está no índice\n",
    "    tokens = message.split()\n",
    "    last_word = tokens[-1] if tokens else None\n",
    "\n",
    "    if last_word not in index:\n",
    "        return \"Desculpe, não entendi sua mensagem.\"\n",
    "\n",
    "    # Obter o ID do último token\n",
    "    current_token_id = index[last_word]\n",
    "    response_ids = []\n",
    "\n",
    "    # Gerar a resposta com base nas probabilidades\n",
    "    while current_token_id != 0:  # Termina quando o token 0 é alcançado\n",
    "        response_ids.append(current_token_id)\n",
    "        next_token_probs = token_probabilities.get(current_token_id, {})\n",
    "\n",
    "        if not next_token_probs:\n",
    "            break  # Se não houver transições, encerra a geração\n",
    "\n",
    "        # Escolher o próximo token com base nas probabilidades\n",
    "        next_token_id = random.choices(\n",
    "            list(next_token_probs.keys()),\n",
    "            weights=list(next_token_probs.values())\n",
    "        )[0]\n",
    "        current_token_id = next_token_id\n",
    "\n",
    "    # Converter os IDs gerados de volta para palavras\n",
    "    reverse_index = {v: k for k, v in index.items()}\n",
    "    response_tokens = [reverse_index[token_id] for token_id in response_ids]\n",
    "\n",
    "    return \" \".join(response_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9917cf22-12ce-415f-81eb-ee2dbe7a0de0",
   "metadata": {},
   "source": [
    "<h3 style=\"color: blue\">5. Testes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4da54-dbfd-44e7-b11d-773819e7e329",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"Olá!\"\n",
    "response = generate_response(user_message, index, token_probabilities)\n",
    "print(f\"Usuário: {user_message}\")\n",
    "print(f\"Modelo: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b390865-e5df-4bd4-857e-e6ac66d55863",
   "metadata": {},
   "source": [
    "<h3 style=\"color: blue\">6. Refinamento</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848ad0f-1e74-4ef3-a090-12fe57258591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a2ab67-051b-4fc0-b520-e1bc9e33fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ngram_probabilities(sequences, n=2):\n",
    "    transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "    transition_probabilities = {}\n",
    "\n",
    "    # Contar transições entre n-gramas\n",
    "    for seq in sequences:\n",
    "        for i in range(len(seq) - n + 1):\n",
    "            ngram = tuple(seq[i:i + n - 1])  # Contexto (n-1 tokens)\n",
    "            next_token = seq[i + n - 1]  # Próximo token\n",
    "            transition_counts[ngram][next_token] += 1\n",
    "\n",
    "    # Calcular probabilidades normalizadas\n",
    "    for ngram, next_tokens in transition_counts.items():\n",
    "        total_transitions = sum(next_tokens.values())\n",
    "        transition_probabilities[ngram] = {\n",
    "            next_token: count / total_transitions\n",
    "            for next_token, count in next_tokens.items()\n",
    "        }\n",
    "\n",
    "    return transition_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412515c6-ff73-4af0-a8e1-1908eaa0ca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_with_context(message, index, token_probabilities, n=2):\n",
    "    tokens = message.split()\n",
    "    if not tokens:\n",
    "        return \"Desculpe, não entendi sua mensagem.\"\n",
    "\n",
    "    # Obter o contexto inicial (n-1 últimos tokens)\n",
    "    context = tuple(tokens[-(n - 1):]) if len(tokens) >= n - 1 else tuple(tokens)\n",
    "    context_ids = [index.get(token, None) for token in context]\n",
    "\n",
    "    if None in context_ids:\n",
    "        return \"Desculpe, não entendi sua mensagem.\"\n",
    "\n",
    "    response_ids = list(context_ids)\n",
    "\n",
    "    # Gerar a resposta com base nas probabilidades de n-gramas\n",
    "    while True:\n",
    "        current_ngram = tuple(response_ids[-(n - 1):])  # Últimos n-1 tokens\n",
    "        next_token_probs = token_probabilities.get(current_ngram, {})\n",
    "\n",
    "        if not next_token_probs:\n",
    "            break  # Se não houver transições, encerra a geração\n",
    "\n",
    "        # Escolher o próximo token com base nas probabilidades\n",
    "        next_token_id = random.choices(\n",
    "            list(next_token_probs.keys()),\n",
    "            weights=list(next_token_probs.values())\n",
    "        )[0]\n",
    "        response_ids.append(next_token_id)\n",
    "\n",
    "        # Termina se o token gerado for o de padding (0)\n",
    "        if next_token_id == 0:\n",
    "            break\n",
    "\n",
    "    # Converter os IDs gerados de volta para palavras\n",
    "    reverse_index = {v: k for k, v in index.items()}\n",
    "    response_tokens = [reverse_index[token_id] for token_id in response_ids if token_id in reverse_index]\n",
    "\n",
    "    return \" \".join(response_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f1e2a-849c-4c54-be9a-7c8641fb8792",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_probabilities = calculate_ngram_probabilities(dataset_ids, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ba1b3-fb8d-4186-b92b-72c3ab3220ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "user_message = \"Oi! Tudo bem? Eu te desjo um bom dia!\"\n",
    "response = generate_response_with_context(user_message, index, token_probabilities, n=2)\n",
    "print(f\"Usuário: {user_message}\")\n",
    "print(f\"Modelo: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f709fd44-4544-41af-9802-ecfaeb794e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_from_message(message, index, tokenized_dataset, dataset_ids, token_probabilities, n=2):\n",
    "    #Salvar nova mensagem no dataset para aprendizado fixo\n",
    "    with open (\"dataset.txt\", \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"{message}\\n\")\n",
    "    \n",
    "    # Tokenizar a nova mensagem\n",
    "    new_tokens = message.split()\n",
    "    \n",
    "    # Atualizar o índice com novos tokens\n",
    "    contador = max(index.values(), default=-1) + 1\n",
    "    for token in new_tokens:\n",
    "        if token not in index:\n",
    "            index[token] = contador\n",
    "            contador += 1\n",
    "\n",
    "    # Atualizar o conjunto de dados tokenizado\n",
    "    new_sentence_ids = [index[token] for token in new_tokens]\n",
    "    tokenized_dataset.append(new_tokens)\n",
    "    dataset_ids.append(new_sentence_ids)\n",
    "\n",
    "    # Recalcular as probabilidades de transição\n",
    "    token_probabilities.update(calculate_ngram_probabilities(dataset_ids, n=n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735a1100-3a2a-475b-8d89-4081225e22aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_message = \"carros são automóveis movidos a combustível\"\n",
    "learn_from_message(new_message, index, tokenized_dataset, dataset_ids, token_probabilities, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5198b2-4a20-4a1e-8d4d-4c1665f3adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar a geração de resposta após o aprendizado\n",
    "user_message = \"O que são carros?\"\n",
    "response = generate_response_with_context(user_message, index, token_probabilities, n=2)\n",
    "print(f\"Usuário: {user_message}\")\n",
    "print(f\"Modelo: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827edff2-f9f2-43c0-864d-1332706ec995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
